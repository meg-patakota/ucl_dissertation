{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import keras_metrics\n",
    "import tensorflow as tf    \n",
    "from tensorflow import keras \n",
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the package\n",
    "final_df = pd.read_csv('/project/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>lens_id</th>\n",
       "      <th>date_published</th>\n",
       "      <th>publication_type</th>\n",
       "      <th>cpc</th>\n",
       "      <th>yo2</th>\n",
       "      <th>abstract.text_sw</th>\n",
       "      <th>biblio.invention_title.text_sw</th>\n",
       "      <th>claims.claims_sw</th>\n",
       "      <th>biblio.invention_title.text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99966</td>\n",
       "      <td>103917</td>\n",
       "      <td>165-470-842-755-208</td>\n",
       "      <td>2016-02-23</td>\n",
       "      <td>GRANTED_PATENT</td>\n",
       "      <td>A61B17/0401,A61B17/0401,A61B2017/0404,A61B2017...</td>\n",
       "      <td>0</td>\n",
       "      <td>device method pushing anchor bore bone provide...</td>\n",
       "      <td>flipp tack pusher</td>\n",
       "      <td>a method for pushing an anchor through a bor...</td>\n",
       "      <td>['flipp', 'tack', 'pusher']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>663454</td>\n",
       "      <td>691679</td>\n",
       "      <td>113-864-851-746-050</td>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>PATENT_APPLICATION</td>\n",
       "      <td>G03B7/097,H04N5/2352,H04N5/2352,H04N5/2353,H04...</td>\n",
       "      <td>0</td>\n",
       "      <td>imaging apparatus includes lens unit body unit...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>an imaging apparatus comprising  a lens uni...</td>\n",
       "      <td>['unknown']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540952</td>\n",
       "      <td>563920</td>\n",
       "      <td>015-657-426-811-531</td>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>GRANTED_PATENT</td>\n",
       "      <td>H04N19/154,H04N19/154,H04N7/17318,H04N7/17318,...</td>\n",
       "      <td>0</td>\n",
       "      <td>method extracting information encoded bit stre...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>a method in a decoding device of adapting a ...</td>\n",
       "      <td>['unknown']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340420</td>\n",
       "      <td>354497</td>\n",
       "      <td>189-965-709-133-536</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>GRANTED_PATENT</td>\n",
       "      <td>C10G47/00,C10G47/00,C10G67/04,C10G67/04,C10G67...</td>\n",
       "      <td>0</td>\n",
       "      <td>hydrocarbon feedstock hydrocracked hydrocracki...</td>\n",
       "      <td>high quality middle distillate production process</td>\n",
       "      <td>a process for producing reduced aromatic hyd...</td>\n",
       "      <td>['high', 'quality', 'middle', 'distillate', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51087</td>\n",
       "      <td>53037</td>\n",
       "      <td>015-709-759-851-330</td>\n",
       "      <td>2016-03-17</td>\n",
       "      <td>PATENT_APPLICATION</td>\n",
       "      <td>H02M3/33507,H02M3/33507,H02M1/32,H02M1/32,</td>\n",
       "      <td>0</td>\n",
       "      <td>operation flybuck converter first output capac...</td>\n",
       "      <td>overcurrent recovery in flybuck converters</td>\n",
       "      <td>a method of operating an isolated buck conv...</td>\n",
       "      <td>['overcurrent', 'recovery', 'in', 'flybuck', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1              lens_id date_published  \\\n",
       "0       99966        103917  165-470-842-755-208     2016-02-23   \n",
       "1      663454        691679  113-864-851-746-050     2016-12-15   \n",
       "2      540952        563920  015-657-426-811-531     2016-10-04   \n",
       "3      340420        354497  189-965-709-133-536     2016-05-10   \n",
       "4       51087         53037  015-709-759-851-330     2016-03-17   \n",
       "\n",
       "     publication_type                                                cpc  yo2  \\\n",
       "0      GRANTED_PATENT  A61B17/0401,A61B17/0401,A61B2017/0404,A61B2017...    0   \n",
       "1  PATENT_APPLICATION  G03B7/097,H04N5/2352,H04N5/2352,H04N5/2353,H04...    0   \n",
       "2      GRANTED_PATENT  H04N19/154,H04N19/154,H04N7/17318,H04N7/17318,...    0   \n",
       "3      GRANTED_PATENT  C10G47/00,C10G47/00,C10G67/04,C10G67/04,C10G67...    0   \n",
       "4  PATENT_APPLICATION         H02M3/33507,H02M3/33507,H02M1/32,H02M1/32,    0   \n",
       "\n",
       "                                    abstract.text_sw  \\\n",
       "0  device method pushing anchor bore bone provide...   \n",
       "1  imaging apparatus includes lens unit body unit...   \n",
       "2  method extracting information encoded bit stre...   \n",
       "3  hydrocarbon feedstock hydrocracked hydrocracki...   \n",
       "4  operation flybuck converter first output capac...   \n",
       "\n",
       "                      biblio.invention_title.text_sw  \\\n",
       "0                                  flipp tack pusher   \n",
       "1                                            unknown   \n",
       "2                                            unknown   \n",
       "3  high quality middle distillate production process   \n",
       "4         overcurrent recovery in flybuck converters   \n",
       "\n",
       "                                    claims.claims_sw  \\\n",
       "0    a method for pushing an anchor through a bor...   \n",
       "1     an imaging apparatus comprising  a lens uni...   \n",
       "2    a method in a decoding device of adapting a ...   \n",
       "3    a process for producing reduced aromatic hyd...   \n",
       "4     a method of operating an isolated buck conv...   \n",
       "\n",
       "                   biblio.invention_title.text_token  \n",
       "0                        ['flipp', 'tack', 'pusher']  \n",
       "1                                        ['unknown']  \n",
       "2                                        ['unknown']  \n",
       "3  ['high', 'quality', 'middle', 'distillate', 'p...  \n",
       "4  ['overcurrent', 'recovery', 'in', 'flybuck', '...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "y = final_df['yo2']\n",
    "X = final_df.drop(columns = 'yo2')\n",
    "X['tac'] = X['biblio.invention_title.text_sw'] + ' ' + X['abstract.text_sw'] + ' ' + X['claims.claims_sw']\n",
    "X['title_abs'] = X['biblio.invention_title.text_sw'] + ' ' + X['abstract.text_sw'] \n",
    "X['title_claims'] = X['biblio.invention_title.text_sw'] + ' ' + X['claims.claims_sw']\n",
    "X['abs_claims'] = X['abstract.text_sw'] + ' ' + X['claims.claims_sw']\n",
    "X['abstract'] = X['abstract.text_sw']\n",
    "X['title'] = X['biblio.invention_title.text_sw']\n",
    "X['claims'] = X['claims.claims_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X['abs_claims'], y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (37500,), (12500,), (12500,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape,trainy.shape,testX.shape,testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_abs_claims.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "save_dataset([trainX,trainy], 'train_abs_claims.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pydot\n",
    "filename = 'train_abs_claims.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(500, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Precision()])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='multichannel_abs_claims.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 18950\n",
      "Vocabulary size: 81326\n",
      "(37500, 18950)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 18950)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 18950)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 18950)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 18950, 100)   8132600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 18950, 100)   8132600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 18950, 100)   8132600     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 18949, 32)    6432        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 18948, 32)    9632        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 18947, 32)    12832       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 18949, 32)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18948, 32)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 18947, 32)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 9474, 32)     0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 9474, 32)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 9473, 32)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 303168)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 303168)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 303136)       0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 909472)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 500)          454736500   concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            501         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 479,163,697\n",
      "Trainable params: 479,163,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 13181s 6s/step - loss: 1.9521 - precision: 0.2128\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 7389s 3s/step - loss: 0.4705 - precision: 0.5444\n",
      "Epoch 3/10\n",
      "1301/2344 [===============>..............] - ETA: 2:09:20 - loss: 0.2433 - precision: 0.7010"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cad39a045580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrains_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrains_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrains_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset(filename)\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trains_X = encode_text(tokenizer, trainLines, length)\n",
    "print(trains_X.shape)\n",
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "class_weights = 1/(trainLabels.value_counts()/len(trainLabels))\n",
    "class_weights = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# fit model\n",
    "model.fit([trains_X,trains_X,trains_X], array(trainLabels), epochs=7, batch_size=16, class_weight=class_weights)\n",
    "\n",
    "#list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# save the model\n",
    "model.save('model_abs_claims.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
